# Model Configuration - Based on Section 2.2 of the paper

# Base Model
model:
  name: "meta-llama/Meta-Llama-3-8B"
  local_files_only: false
  torch_dtype: "float16"

# LoRA Configuration (Section 2.2.3)
lora:
  r: 16  # Low-rank dimension
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization (Section 2.2.3)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# Training Configuration
training:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_steps: 100
  max_grad_norm: 0.3
  optimizer: "paged_adamw_8bit"
  lr_scheduler_type: "cosine"
  logging_steps: 10
  save_steps: 100
  max_seq_length: 4096

# Few-shot Configuration (Section 2.3.1)
prompting:
  num_shots: 3
  temperature: 0.1
  max_new_tokens: 512
  top_p: 0.95
  do_sample: false

# Validation Configuration (Section 2.3.2)
validation:
  rag:
    top_k: 3
    similarity_threshold: 0.7
    embedding_model: "intfloat/multilingual-e5-large"
    embedding_dim: 1024
    faiss_index_type: "IndexFlatIP"
  
  cosine_similarity:
    threshold_percentile: 5
    similarity_threshold: 0.82
  
  hitl:
    flagged_sample_rate: 1.0  # Review all flagged
    random_sample_rate: 0.1   # Review 10% of non-flagged

# Prediction Model Configuration (Section 2.4)
prediction:
  test_size: 0.2
  validation_size: 0.2
  random_state: 42
  smote:
    sampling_strategy: "auto"
    k_neighbors: 5
  
  models:
    logistic_regression:
      penalty: "l2"
      C: 1.0
      max_iter: 1000
    
    catboost:
      iterations: 1000
      learning_rate: 0.03
      depth: 6
      l2_leaf_reg: 3
      random_seed: 42
    
    tabpfn:
      N_ensemble_configurations: 32
